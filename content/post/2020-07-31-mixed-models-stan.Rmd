---
title: "Building mixed effects models in Stan"
author: "Jake Wittman"
date: '2020-07-31'
tags:
- simulation
- Bayesian analysis
categories: self-resource
---
```{r, include=FALSE}
knitr::opts_chunk$set(eval = FALSE)
```


This post is for my own edification. I'm having trouble grokking writing a mixed-effects model in Stan using matrices just by looking at example code. 

```{r}
# Set up libraries
library(tidyverse)
library(rstan)
library(brms)
library(bayesplot)
```


```{r}
# Simulate snake length data - based on code from Kery 2010
set.seed(314)
# Sample size values
n_groups <- 56 # Number of groups/populations
n_samples <- 10 # Number of samples in each population (note to self: try doing this later with varying n_samples in each pop)
n <- n_groups * n_samples
group_indicator <- gl(n = n_groups, k = n_samples)

# Body length (cm)

original_length <- runif(n, 45, 70)
standard_length <- scale(original_length)
hist(standard_length)

# Build the design matrix without intercept
Xmat <- model.matrix(~group_indicator * standard_length - 1)

# Hyperparameters
intercept_mean <- 230 # mu alpha
intercept_sd <- 20 # sigma alpha
slope_mean <- 60 # mu beta
slope_sd <- 30 # sigma beta

# Parameters
intercept_effects <- rnorm(n = n_groups, mean = intercept_mean, sd = intercept_sd)
slope_effects <- rnorm(n = n_groups, mean = slope_mean, sd = slope_sd)
all_effects <- c(intercept_effects, slope_effects)

# Simulate response variable
lin_pred <- Xmat %*% all_effects
errors <- rnorm(n = n, mean = 0, sd = 30)
mass <- lin_pred + errors

# Create a data frame to visualize
# adding a random indicator for a second random effect for some experimentation
# and a random_x variable for expeirmentation too
dat <- data.frame(mass = mass,
                  standard_length = standard_length,
                  group_indicator = group_indicator,
                  random_indicator = as.factor(sample(1:10, size = n, replace = T)),
                  random_x = runif(n = n, 0, 1))

ggplot(dat) +
  geom_point(aes(x = standard_length, y = mass)) +
  facet_wrap(~group_indicator)


```


Now that I've got some data, let's create a simple Stan model. This is cobbled together from looking at the the Stan code generated by brms (using `stancode(model_name)`). Ok, it's basically just recreating the Stan code underlying a brms model. But it ended up being helpful for me to type all this out myself, change a few things, etc.

Another note: brms standardizes the group level effects then transforms them. I think this is part of the non-centered parameterization.

```{stan, output.var = "rand_int_model"}
data {
  // Population level data
  int<lower=1> N_obs; // # of observations. This is 56 * 10 = 560
  vector[N_obs] Y; // Response variable
  int<lower=1> K; // # of predictors + intercept
  matrix[N_obs, K] X; // Population level design matrix
  
  // Group level data
  int<lower=1> N_groups_1; // # of levels for the random effects variable. This should be 56 in my example
  int<lower=1> J_1[N_obs]; // Indicator vector identifying which group each obs. belongs to
  int<lower=1> M_1; // # of coefficients per group. If random intercepts model = 1. If random int. and slopes, then   equal to the number of variables with varying slopes + 1 for intercept
  vector[N_obs] Z_1_1; // Group level predictor values (?? set to all 1s when checking brms data)
  
}

transformed data{
  // This removes the intercept from the design matrix and centers the design matrix
  // This includes centering if the variable is a factor.
  // If you center your variables before doing this, this part becomes
  // unnecessary.
  int Kc = K - 1; // Remove the intercept from the count of variables
  matrix[N_obs, Kc] Xc; // Create the centered X model matrix
  vector[Kc] means_X; // column means of X before centering
  // Remove the column for the intercept
  for (i in 2:K) {
    means_X[i - 1] = mean(X[, i]);
    Xc[, i - 1] = X[, i];
  }
}


parameters {
  real Intercept; 
  vector[Kc] b; // population-level parameters
  real<lower=0> sigma; // residual SD
  vector<lower=0>[M_1] sd_1; // group-level standard deviations
  vector[N_groups_1] z_1[M_1]; // standardized group-level effects

}

transformed parameters {
  vector[N_groups_1] r_1_1; // actual group_level effects
  r_1_1 = (sd_1[1] * (z_1[1]));
}

// The model
model {

// initialize linear predictor

  vector[N_obs] mu = Intercept + Xc * b;
  // Add the random effects component to the mean observed value
  for (n in 1:N_obs) {
  // The Z_1_1 below becomes more important when modeling random slopes (i.e 
  // the values take on numbers other than 1).
    mu[n] += r_1_1[J_1[n]] * Z_1_1[n]; 
  }

  // priors
  // One can mix the target += lpmf notation with the ~ sampling notation
  b ~ normal(0 , 50);
  Intercept ~ normal(0, 100);
  sigma ~ normal(0, 50);
  sd_1 ~ normal(0, 50);
  z_1[1] ~ std_normal();
  Y ~ normal(mu, sigma);
}

generated quantities{
 real b_Intercept = Intercept - dot_product(means_X, b);

}




```

Ok so that's the model code. Let's try it.

```{r, include = FALSE}
rand_int_model <- "data {
  // Population level data
  int<lower=1> N_obs; // # of observations. This is 56 * 10 = 560
  vector[N_obs] Y; // Response variable
  int<lower=1> K; // # of predictors + intercept
  matrix[N_obs, K] X; // Population level design matrix
  
  // Group level data
  int<lower=1> N_groups_1; // # of levels for the random effects variable. This should be 56 in my example
  int<lower=1> J_1[N_obs]; // Indicator vector identifying which group each obs. belongs to
  int<lower=1> M_1; // # of coefficients per group. If random intercepts model = 1. If random int. and slopes, then   equal to the number of variables with varying slopes + 1 for intercept
  vector[N_obs] Z_1_1; // Group level predictor values (?? set to all 1s when checking brms data)
  
}

transformed data{
  // This removes the intercept from the design matrix and centers the design matrix
  // This includes centering if the variable is a factor.
  // If you center your variables before doing this, this part becomes
  // unnecessary.
  int Kc = K - 1; // Remove the intercept from the count of variables
  matrix[N_obs, Kc] Xc; // Create the centered X model matrix
  vector[Kc] means_X; // column means of X before centering
  // Remove the column for the intercept
  for (i in 2:K) {
    means_X[i - 1] = mean(X[, i]);
    Xc[, i - 1] = X[, i];
  }
}


parameters {
  real Intercept; 
  vector[Kc] b; // population-level parameters
  real<lower=0> sigma; // residual SD
  vector<lower=0>[M_1] sd_1; // group-level standard deviations
  vector[N_groups_1] z_1[M_1]; // standardized group-level effects

}

transformed parameters {
  vector[N_groups_1] r_1_1; // actual group_level effects
  r_1_1 = (sd_1[1] * (z_1[1]));
}

// The model
model {

// initialize linear predictor

  vector[N_obs] mu = Intercept + Xc * b;
  // Add the random effects component to the mean observed value
  for (n in 1:N_obs) {
  // The Z_1_1 below becomes more important when modeling random slopes (i.e 
  // the values take on numbers other than 1).
    mu[n] += r_1_1[J_1[n]] * Z_1_1[n]; 
  }

  // priors
  // One can mix the target += lpmf notation with the ~ sampling notation
  b ~ normal(0 , 50);
  Intercept ~ normal(0, 100);
  sigma ~ normal(0, 50);
  sd_1 ~ normal(0, 50);
  z_1[1] ~ std_normal();
  Y ~ normal(mu, sigma);
}

generated quantities{
 real b_Intercept = Intercept - dot_product(means_X, b);

}



"
```

The Stan code above is commented fairly heavily by me to cover what each part is doing.

```{r}
# I could use make_standata, but I want to practice getting everything together
# by hand
rand_int_data <- list(N_obs = n,
                      Y = as.vector(mass),
                      K = 2, # number of predictors including intercept
                      X = model.matrix(~ standard_length),
                      Z_1_1 = rep(1, n),
                      J_1 = as.numeric(group_indicator),
                      N_groups_1 = n_groups,
                      M_1 = 1 # random int model so M_1 = 1
                      )
rand_int_samples <- stan(model_code = rand_int_model,
                         #file = here::here("content/post/test.stan"),
                         data = rand_int_data,
                         chains = 3,
                         cores = 3,
                         seed = 314)

brms_rand_int <- brm(mass ~ standard_length + (1 | group_indicator),
                     data = dat,
                     chains = 3,
                     cores = 3,
                     seed = 314,
                     prior = c(prior(normal(0, 50), b),
                               prior(normal(0, 100), Intercept),
                               prior(normal(0, 50), sigma),
                               prior(normal(0, 50), sd)))

summary(rand_int_samples)$summary %>% round(., digits = 4)

summary(brms_rand_int)

# Parameter posterior densities are similar.
mcmc_dens(brms_rand_int, pars = vars(contains("b")))
mcmc_dens(rand_int_samples, pars = vars(contains("b")))
```

Unsurprisngly, the "custom" coded model gives the same results. Unsurprising because I basically just copied the brms code in an attempt to understand it. I played around with it, changing parts and taking out parts to see what broke and what still worked, which is how I got most of my comments. 

Now, time to fit a model with random slopes since that's how I simmed the data. The data were not simulated with correlation between the intercept and slopes, so we'll start by fitting a model that does not include that correlation.

```{stan, output.var = "rand_slopes_model"}
// Much of the data block is the same but we now have z_1_2 for the slopes
functions {
}
data {
  int<lower=1> N;  // number of observations
  vector[N] Y;  // response variable
  int<lower=1> K;  // number of population-level effects
  matrix[N, K] X;  // population-level design matrix
  // data for group-level effects of ID 1
  int<lower=1> N_1;  // number of grouping levels
  int<lower=1> M_1;  // number of coefficients per level
  int<lower=1> J_1[N];  // grouping indicator per observation
  // group-level predictor values
  vector[N] Z_1_1;
  vector[N] Z_1_2;
}
transformed data {
  int Kc = K - 1;
  matrix[N, Kc] Xc;  // centered version of X without an intercept
  vector[Kc] means_X;  // column means of X before centering
  for (i in 2:K) {
    means_X[i - 1] = mean(X[, i]);
    Xc[, i - 1] = X[, i] - means_X[i - 1];
  }
}
parameters {
  vector[Kc] b;  // population-level effects
  real Intercept;  // temporary intercept for centered predictors
  real<lower=0> sigma;  // residual SD
  vector<lower=0>[M_1] sd_1;  // group-level standard deviations
  vector[N_1] z_1[M_1];  // standardized group-level effects
}
transformed parameters {
  vector[N_1] r_1_1;  // actual group-level effects
  vector[N_1] r_1_2;  // actual group-level effects
  r_1_1 = (sd_1[1] * (z_1[1]));
  r_1_2 = (sd_1[2] * (z_1[2]));
}
model {
  // initialize linear predictor term
  vector[N] mu = Intercept + Xc * b;
  for (n in 1:N) {
    // add more terms to the linear predictor
    mu[n] += r_1_1[J_1[n]] * Z_1_1[n] + r_1_2[J_1[n]] * Z_1_2[n];
  }
  // priors including all constants
  target += normal_lpdf(b | 0, 50);
  target += normal_lpdf(Intercept | 0, 100);
  target += normal_lpdf(sigma | 0, 50)
    - 1 * normal_lccdf(0 | 0, 50);
  target += normal_lpdf(sd_1 | 0, 50)
    - 2 * normal_lccdf(0 | 0, 50);
  target += std_normal_lpdf(z_1[1]);
  target += std_normal_lpdf(z_1[2]);
}
generated quantities {
  // actual population-level intercept
  real b_Intercept = Intercept - dot_product(means_X, b);
}
```


```{r, include = FALSE}
rand_slope_model <- "
data {
  int<lower=1> N;  // number of observations
  vector[N] Y;  // response variable
  int<lower=1> K;  // number of population-level effects
  matrix[N, K] X;  // population-level design matrix
  // data for group-level effects of ID 1
  int<lower=1> N_1;  // number of grouping levels
  int<lower=1> M_1;  // number of coefficients per level
  int<lower=1> J_1[N];  // grouping indicator per observation
  // group-level predictor values
  vector[N] Z_1_1;
  vector[N] Z_1_2;

}
transformed data {
  int Kc = K - 1;
  matrix[N, Kc] Xc;  // centered version of X without an intercept
  vector[Kc] means_X;  // column means of X before centering
  for (i in 2:K) {
    means_X[i - 1] = mean(X[, i]);
    Xc[, i - 1] = X[, i] - means_X[i - 1];
  }
}
parameters {
  vector[Kc] b;  // population-level effects
  real Intercept;  // temporary intercept for centered predictors
  real<lower=0> sigma;  // residual SD
  vector<lower=0>[M_1] sd_1;  // group-level standard deviations
  vector[N_1] z_1[M_1];  // standardized group-level effects
}
transformed parameters {
  vector[N_1] r_1_1;  // actual group-level effects
  vector[N_1] r_1_2;  // actual group-level effects
  r_1_1 = (sd_1[1] * (z_1[1]));
  r_1_2 = (sd_1[2] * (z_1[2]));
}
model {
  // initialize linear predictor term
  vector[N] mu = Intercept + Xc * b;
  for (n in 1:N) {
    // add more terms to the linear predictor
    mu[n] += r_1_1[J_1[n]] * Z_1_1[n] + r_1_2[J_1[n]] * Z_1_2[n];
  }
  // priors including all constants
  target += normal_lpdf(b | 0, 50);
  target += normal_lpdf(Intercept | 0, 100);
  target += normal_lpdf(sigma | 0, 50)
    - 1 * normal_lccdf(0 | 0, 50);
  target += normal_lpdf(sd_1 | 0, 50)
    - 2 * normal_lccdf(0 | 0, 50);
  target += std_normal_lpdf(z_1[1]);
  target += std_normal_lpdf(z_1[2]);
  // likelihood including all constants

 target += normal_lpdf(Y | mu, sigma);

}
generated quantities {
  // actual population-level intercept
  real b_Intercept = Intercept - dot_product(means_X, b);
}

"
```


One note I want to make is that the random effects get a single vector, much like the fixed effects do. I don't have to add individual parameters in the parameter and model section, I can incorporate them all into a single vector.

So adding random slopes without correlation between the slopes and intercepts is only a few more lines of code.

```{r}
# I could use make_standata, but I want to practice getting everything together
# by hand
rand_slope_data <- list(N = n,
                      Y = as.vector(mass),
                      K = 2, # number of predictors including intercept
                      X = model.matrix(~ standard_length),
                      Z_1_1 = rep(1, n),
                      Z_1_2 = dat$standard_length,
                      J_1 = as.numeric(group_indicator),
                      N_1 = n_groups,
                      M_1 = 2 # random slopes model so M_1 = 2
                      )
rand_slope_samples <- stan(model_code = rand_slope_model,
                         data = rand_slope_data,
                         chains = 3,
                         cores = 3,
                         seed = 314)


brms_rand_slope <- brm(mass ~ standard_length + (standard_length || group_indicator),
                     data = dat,
                     chains = 3,
                     cores = 3,
                     seed = 314,
                     prior = c(prior(normal(0, 50), b),
                               prior(normal(0, 100), Intercept),
                               prior(normal(0, 50), sigma),
                               prior(normal(0, 50), sd)))

mcmc_dens(brms_rand_slope, pars = vars(contains("b")))
mcmc_dens(rand_slope_samples, pars = vars(contains("b")))
```


Now, let's try simulating data with correlation between the random slopes and intercepts and modeling that way.

```{r}
library(MASS)
# Need a parameter for covariance
intercept_slope_covariance <- 10

mu_vector <- c(intercept_mean, slope_mean)
var_cov_matrix <- matrix(c(intercept_sd^2, intercept_slope_covariance,
                           intercept_slope_covariance, slope_sd^2), 2, 2)
effects <- mvrnorm(n = n_groups, mu = mu_vector, Sigma = var_cov_matrix)
all_effects <- c(effects[, 1], effects[, 2])

lin_pred <- Xmat[,] %*% all_effects
eps <- rnorm(n = n, mean = 0, sd = 30)
mass <- lin_pred + eps

# Create a new data frame
dat <- data.frame(mass = mass,
                  standard_length = standard_length,
                  group_indicator = group_indicator,
                  random_indicator = as.factor(sample(1:10, size = n, replace = T)),
                  random_x = runif(n = n, 0, 1))

ggplot(dat) +
  geom_point(aes(x = standard_length, y = mass)) +
  facet_wrap(~group_indicator)

```


Now here's the Stan code. It adds a few pieces that weren't present when we didn't model correlation between the slope and intercept. The first is the `NC_1` which is the number of group-level correlations. The standardized group level effects `z_1` is now a matrix object (although in the last example it was a multi-dimensional vector. I'm not sure how the two are different in Stan). We also have a parameter for the correlation matrix (the Cholesky factor of the correlation matrix). From what I understand (which is little), the Cholesky factor is a decomposition of the actual matrix and it's useful here for the computational properties it provides? I think?



```{stan, output.var = "corr_mod"}
data {
  int<lower=1> N;  // number of observations
  vector[N] Y;  // response variable
  int<lower=1> K;  // number of population-level effects
  matrix[N, K] X;  // population-level design matrix
  // data for group-level effects of ID 1
  int<lower=1> N_1;  // number of grouping levels
  int<lower=1> M_1;  // number of coefficients per level
  int<lower=1> J_1[N];  // grouping indicator per observation
  // group-level predictor values
  vector[N] Z_1_1;
  vector[N] Z_1_2;
  int<lower=1> NC_1;  // number of group-level correlations
}
transformed data {
  int Kc = K - 1;
  matrix[N, Kc] Xc;  // centered version of X without an intercept
  vector[Kc] means_X;  // column means of X before centering
  for (i in 2:K) {
    means_X[i - 1] = mean(X[, i]);
    Xc[, i - 1] = X[, i] - means_X[i - 1];
  }
}
parameters {
  vector[Kc] b;  // population-level effects
  real Intercept;  // temporary intercept for centered predictors
  real<lower=0> sigma;  // residual SD
  vector<lower=0>[M_1] sd_1;  // group-level standard deviations
  matrix[M_1, N_1] z_1;  // standardized group-level effects
  cholesky_factor_corr[M_1] L_1;  // cholesky factor of correlation matrix
}
transformed parameters {
  matrix[N_1, M_1] r_1;  // actual group-level effects
  // using vectors speeds up indexing in loops
  vector[N_1] r_1_1;
  vector[N_1] r_1_2;
  // compute actual group-level effects
  // returns the product of the diagonal matrix from the sd vector and the
  // matrix L (which is the covariance matrix, I think), multiplied by the
  // standardized group-level effects
  r_1 = (diag_pre_multiply(sd_1, L_1) * z_1)'; 
  r_1_1 = r_1[, 1];
  r_1_2 = r_1[, 2];
}
model {
  // initialize linear predictor term
  vector[N] mu = Intercept + Xc * b;
  for (n in 1:N) {
    // add more terms to the linear predictor
    mu[n] += r_1_1[J_1[n]] * Z_1_1[n] + r_1_2[J_1[n]] * Z_1_2[n];
  }
  // priors including all constants
  target += normal_lpdf(b | 0, 50);
  target += normal_lpdf(Intercept | 0, 100);
  target += normal_lpdf(sigma | 0, 50)
    - 1 * normal_lccdf(0 | 0, 50);
  target += normal_lpdf(sd_1 | 0, 50)
    - 2 * normal_lccdf(0 | 0, 50);
  target += std_normal_lpdf(to_vector(z_1));
  // Need a prior on the covariance matrix
  // Per the brms manual an LKJ prior of 1 means all correlations are equally
  // likely. If > 1, extreme correlations are less likely, while < 1 but > 0 
  // extreme correlations become more likely.
  target += lkj_corr_cholesky_lpdf(L_1 | 1);
  // likelihood including all constants
  target += normal_lpdf(Y | mu, sigma);
  
}
generated quantities {
  // actual population-level intercept
  real b_Intercept = Intercept - dot_product(means_X, b);
  // compute group-level correlations
  // The Cholesky matrix is (I think) a lower triangular matrix, using this
  // multiplies itself by it's transpose to give the full covariance matrix?
  // This is necessary because the decomposition turns the 1s on the diagonal of
  // the covariance matrix to non-1 values (so that multiplying L_1 * (L_1)^T 
  // gives the proper covariance matrix).
  corr_matrix[M_1] Cor_1 = multiply_lower_tri_self_transpose(L_1);
  vector<lower=-1,upper=1>[NC_1] cor_1;
  // extract upper diagonal of correlation matrix
  for (k in 1:M_1) {
    for (j in 1:(k - 1)) {
      cor_1[choose(k - 1, 2) + j] = Cor_1[j, k];
    }
  }
  

}


```



```{r, include = FALSE}
corr_mod <- "
data {
  int<lower=1> N;  // number of observations
  vector[N] Y;  // response variable
  int<lower=1> K;  // number of population-level effects
  matrix[N, K] X;  // population-level design matrix
  // data for group-level effects of ID 1
  int<lower=1> N_1;  // number of grouping levels
  int<lower=1> M_1;  // number of coefficients per level
  int<lower=1> J_1[N];  // grouping indicator per observation
  // group-level predictor values
  vector[N] Z_1_1;
  vector[N] Z_1_2;
  int<lower=1> NC_1;  // number of group-level correlations
}
transformed data {
  int Kc = K - 1;
  matrix[N, Kc] Xc;  // centered version of X without an intercept
  vector[Kc] means_X;  // column means of X before centering
  for (i in 2:K) {
    means_X[i - 1] = mean(X[, i]);
    Xc[, i - 1] = X[, i] - means_X[i - 1];
  }
}
parameters {
  vector[Kc] b;  // population-level effects
  real Intercept;  // temporary intercept for centered predictors
  real<lower=0> sigma;  // residual SD
  vector<lower=0>[M_1] sd_1;  // group-level standard deviations
  matrix[M_1, N_1] z_1;  // standardized group-level effects
  cholesky_factor_corr[M_1] L_1;  // cholesky factor of correlation matrix
}
transformed parameters {
  matrix[N_1, M_1] r_1;  // actual group-level effects
  // using vectors speeds up indexing in loops
  vector[N_1] r_1_1;
  vector[N_1] r_1_2;
  // compute actual group-level effects
  // returns the product of the diagonal matrix from the sd vector and the
  // matrix L (which is the covariance matrix, I think), multiplied by the
  // standardized group-level effects
  r_1 = (diag_pre_multiply(sd_1, L_1) * z_1)'; 
  r_1_1 = r_1[, 1];
  r_1_2 = r_1[, 2];
}
model {
  // initialize linear predictor term
  vector[N] mu = Intercept + Xc * b;
  for (n in 1:N) {
    // add more terms to the linear predictor
    mu[n] += r_1_1[J_1[n]] * Z_1_1[n] + r_1_2[J_1[n]] * Z_1_2[n];
  }
  // priors including all constants
  target += normal_lpdf(b | 0, 50);
  target += normal_lpdf(Intercept | 0, 100);
  target += normal_lpdf(sigma | 0, 50)
    - 1 * normal_lccdf(0 | 0, 50);
  target += normal_lpdf(sd_1 | 0, 50)
    - 2 * normal_lccdf(0 | 0, 50);
  target += std_normal_lpdf(to_vector(z_1));
  // Need a prior on the covariance matrix
  // Per the brms manual an LKJ prior of 1 means all correlations are equally
  // likely. If > 1, extreme correlations are less likely, while < 1 but > 0 
  // extreme correlations become more likely.
  target += lkj_corr_cholesky_lpdf(L_1 | 1);
  // likelihood including all constants
  target += normal_lpdf(Y | mu, sigma);
  
}
generated quantities {
  // actual population-level intercept
  real b_Intercept = Intercept - dot_product(means_X, b);
  // compute group-level correlations
  // The Cholesky matrix is (I think) a lower triangular matrix, using this
  // multiplies itself by it's transpose to give the full covariance matrix?
  // This is necessary because the decomposition turns the 1s on the diagonal of
  // the covariance matrix to non-1 values (so that multiplying L_1 * (L_1)^T 
  // gives the proper covariance matrix).
  corr_matrix[M_1] Cor_1 = multiply_lower_tri_self_transpose(L_1);
  vector<lower=-1,upper=1>[NC_1] cor_1;
  // extract upper diagonal of correlation matrix
  for (k in 1:M_1) {
    for (j in 1:(k - 1)) {
      cor_1[choose(k - 1, 2) + j] = Cor_1[j, k];
    }
  }
  

}
"

```


```{r}
corr_mod_data <- list(N = n,
                      Y = as.vector(mass),
                      K = 2, # number of predictors including intercept
                      X = model.matrix(~ standard_length),
                      Z_1_1 = rep(1, n),
                      Z_1_2 = dat$standard_length,
                      J_1 = as.numeric(group_indicator),
                      N_1 = n_groups,
                      M_1 = 2, # random slopes model so M_1 = 2
                      NC_1 = 1,
                      prior_only = 0
                      )
cor_mod_samples <- stan(model_code = corr_mod,
                         #file = here::here("content/post/test.stan"),
                         data = corr_mod_data,
                         chains = 3,
                         cores = 3,
                        iter = 20000,
                         seed = 314)


brms_corr_mod <- brm(mass ~ standard_length + (standard_length | group_indicator),
                     data = dat,
                     chains = 3,
                     cores = 3,
                     seed = 314,
                     prior = c(prior(normal(0, 50), b),
                               prior(normal(0, 100), Intercept),
                               prior(normal(0, 50), sigma),
                               prior(normal(0, 50), sd)))

mcmc_dens(cor_mod_samples2, pars = vars(contains("b")))
mcmc_dens(brms_corr_mod, pars = vars(contains("b")))
```

Running the model through rstan, it throws some warnings. These are tied with the Cholesky matrix and the covariance matrix - I think they have something to do with the fact that the Cholesky matrix is a lower triangular matrix, and aren't actually important warnings. The brms package must be set up to ignore those warnings when returning the summary. 

Now I want to know how the random slopes model changes as I add more variables and/or more grouping factors. 

```{r}
# Add one more covariate in the random effects
brms_corr_mod2 <- brm(mass ~ standard_length + random_x + (standard_length + random_x | group_indicator),
                     data = dat,
                     chains = 3,
                     cores = 3,
                     seed = 314,
                     prior = c(prior(normal(0, 50), b),
                               prior(normal(0, 100), Intercept),
                               prior(normal(0, 50), sigma),
                               prior(normal(0, 50), sd)))

stancode(brms_corr_mod2)
standata(brms_corr_mod2)
```

As far as the Stancode, we get an additional `Z_1_3` vector in the data section. The pattern appears to be `Z_X_Y` where X is a number indicating which grouping variable is being used for a random effect and Y is a number corresponding to a coefficient at the group_level (1 is for a random intercept, 2 is for the first covariate, 3 is for the second, etc.). The matrix `z_1` represents all of the group-level (i.e. random) effects for grouping variable 1.

In the data being sent to the Stan code, `NC_1` is now set to 3. This is for the number of covariance parameters. In this model there are 3, the covariance of the intercept and covariate 1, covariance of intercept and covariate 2, and the covariance of covariates 1 and 2. `M_1` is the number of parameters being estimated for group 1. In this model it is 3, in the previous models it was 2 (random int + random slope) and 1 (random int only).

```{r}
brms_corr_mod3 <- brm(mass ~ standard_length + (standard_length | group_indicator) + (standard_length | random_indicator),
                     data = dat,
                     chains = 3,
                     cores = 3,
                     seed = 314,
                     prior = c(prior(normal(0, 50), b),
                               prior(normal(0, 100), Intercept),
                               prior(normal(0, 50), sigma),
                               prior(normal(0, 50), sd)))
stancode(brms_corr_mod3)
standata(brms_corr_mod3)
```


Adding a second grouping factor simply doubles the relevant data and parameters sections (and associated priors and sampling statements in the model). So we have `Z_2_Y`, a second `L_X` cholesky factor correlation matrix, and variables like `N_2`, `M_2` and `J_2`. 

Now, if we have a "ragged" data structure, where members of a group have a different number of observations, does that change the underlying code?

```{r}
dat_sub <- slice_sample(dat, prop = 0.95)

brms_sub_mod <- brm(mass ~ standard_length + (standard_length | group_indicator) + (standard_length | random_indicator),
                     data = dat,
                     chains = 3,
                     cores = 3,
                     seed = 314,
                     prior = c(prior(normal(0, 50), b),
                               prior(normal(0, 100), Intercept),
                               prior(normal(0, 50), sigma),
                               prior(normal(0, 50), sd)))
identical(stancode(brms_sub_mod), stancode(brms_corr_mod3))
```

The result of the identical function shows us that the underlying Stan code is identical between the models. They're written in such a way that different numbers of observations within a group do not cause trouble.